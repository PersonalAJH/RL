1. Machine Learning 의 종류에는 비지도학습, 지도학습, 강화학습이 존재
    1.1 비지도 학습의 예시(CNN, GNN)
    1.2 지도 학습의 예시(생성형 AI, 클러스터링)
    1.3 강화학습

강화학습 
1. 순차적 의사결정 문제 
    1.1 "순차적"인 의사결정을 통해서(연이은 동작을 통해서) 상황이 바뀌고 보상을 획득
    1.2 예시(주식 투자, 운전, 게임)

2. 보상
    2.1 보상(Reward)란 의사결정을 얼마나 잘하고 있는지 알려주는 신호이다.
    2.2 그리고 강화학습의 목적은 과정에서 받은 보상의 총합, 즉 누적보상(cumulative Reward)을 최대화 하는 것이다. 
    2.3 보상의 특징은 "어떻게" 에 대한 정보를 담고 있지 않고 "얼마나" 잘하는지를 아는것이다.   
        - 따라서 어떻게 동작을 하는지를 알려주는 것이 아니라 그냥 보상을 얼마나 줌으로써 전에 했던 행동이 좋았던 행동이었다 를 깨달는 학습
    2.4 보상의 크기는 Scalar 이어야 한다. 방향성에 대한 정보(벡터) 가 아니라 스칼라여야 이 목표가 최대가 되도록함
        - 스칼라가 되지 않는다면 스칼라화 시켜야한다.
    2.5 희소하고 지연된 보상
        - 보상의 세번째 특징은희소(sparse)할 수 있으며 지연(delay)될수 있다. 
        - 행동과 보상이 1:1로 귀결될 수 있다면 훨씬 더 쉬워지겠지만 보통의 경우는 그렇지 않는다.(바둑을 예시로 보면 바둑 250수를 모두 두어야만 이겼는지 졌는지를 판단할 수 있다.)
        -  보상이 희소하고 학습이 어려워 지기 때문에 이런 문제를 해결하기 위해 강화학습 연구에서도 Value Network 등 다양한 아이디어가 있다. 

3. 에이전트와 환경
    3.1 에이전트는 강화학습의 주인공이자 주체, 에이전트의 입장에서는 하나의 루프에서 3개의 단계로 이루어져있다.
        1. 현재 상황 S(t)에서 어떤 액션을 해야 할지 a(t)를 결정
        2. 결정된 행동 a(t)를 환경으로 보냄
        3. 환경으로부터 그에 따른 보상과 다음 상태의 정보를 받음

    3.2 환경은 에이전트를 제외한 모든 요소이며 현재 상태에 대한 모든 정보를 숫자로 기록해 놓은 것. 환경이 하는 일은 4가지로 이루어져 있다.
        1. 에이전트로 부터 받은 액션 a(t)를 통해서 상태 변화를 일으킴
        2. 그 결과 상태는 s(t) -> s(t+1) 로 바뀜
        3. 에이전트에게 줄 보상 r(t+1)eh gkaRp rPtks
        4. s(t+1) 과 r(t+1)을 에이전트에게 전달 

    3.3 이와같이 에이전트가 s(t) 에서 a(t)를 시행하고 이를 통해 환경이 s(t+1) 로 바뀌면, 즉 에이전트와 환경이 한번 상호작용하면 하나의 루프가 끝남 