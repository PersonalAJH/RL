크게 3개로 나누어서 설명
1. Markov Process
2. Markov Reward Process
3. Markov Decision Process(MDP)


1. Markov Process
특징
    - 마르코프 속성: 현재 상태가 미래 상태를 결정하는 데 필요한 모든 정보를 가지고 있고, 이전 상태에 대해서는 기억하지 않는 성질을 말합니다.
    - 상태 공간: 시스템이 존재할 수 있는 모든 가능한 상태들의 집합입니다.
    - 확률 전이 행렬: 한 상태에서 다른 상태로 전이될 확률을 나타내는 행렬로, 이 행렬의 각 요소는 현재 상태에서 다음 상태로 전이될  확률을 나타냅니다.

예시에서는 아이가 자기 위한 상태를 s0 ~s5 6단계로 나누어서 설명했으며 각 단계에서 다른 단계로 이동할 수 있는 확률이 존재함(전이 확률 행렬)

*** 마르코프는 현재 상태의 정보가 미래를 결정하기 떄문에 이전의 무엇을 했는지는 궁금하지 않다 *** ->예시로는 체스(현재 상태에 따라서만 미래 결정이 정해짐), 자동차 운전


2. Markov Reward Process
구성 요소
    1. 상태 집합 (State Space, S): MRP는 상태 집합 𝑆 S에서 정의됩니다. 상태는 시스템이 특정 시점에서 놓여 있는 상황을 나타냅니다.
    
    2. 전이 확률 (Transition Probability, P) : 상태 S 에서 다른 상태 S' 로 이동할 확률, 마르코프 프로세스와 동일'

    3. 보상 함수 (Reward Function, R) : 상태 s에 도달했을 때 얻는 보상입니다.
        - Reward 가 항상 동일 하지 않을 수 있기 때문에 기댓값을 사용한다.

    4. 감가율 (Discount Factor, γ) : 래의 보상을 현재 가치로 환산하는데 사용됩니다. 0~1 사이의  실수로 이루어져 있다. 
        - 수학적 편리성 : 리턴값이 무한이 될 수 있는걸 방지할 수 있으며 만약 무한이 될경우 어느쪽이 더 좋을 지 비교가 어렵다.
        - 사람의 선호 반영 : 바로 받는게 좋다
        - 미래에 대한 불확실성 반영 : 한국이 망할수도 있대. 

MRP에서 각 상태의 밸류 평가하기 
    - 현재 상태의 Value는 그 이후의 결과를 더 잘받는 위치에 있기에 나오는 value 이다.

에피소드의 샘플링
    - 한번의 에피소드를 쭉 해보고 마지막에 결과를 확인하는 방법(S0~S5 까지 모든 상태에서 계속 에피소드를 돌려보고 S0~S5 의 각 상태에 대한 value 의 기댓값을 측정하는 방법)
    - 샘플링을 통해서 어떤값을 유추하는 방법론을 일컬어 Monte-Carlo 접근법이라고 한다. 

3. Markov Decision Process(MDP)
정책
    - 정책은 상태 s에서 에이전트가 취할 행동 a를 결정하는 함수로 정의됩니다. 일반적으로 다음과 같이 표현됩니다.
    - MDP 에서 에이전트는 최적의 정책을 찾아내는 역할을 진행한다. 


